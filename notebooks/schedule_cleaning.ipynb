{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6c3475",
   "metadata": {},
   "source": [
    "Ensure no data leaking by replacing post game stats with previous averaging"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 247,
=======
   "execution_count": 47,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "7dd0b30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>Winner</th>\n",
       "      <th>home_pass_cmp</th>\n",
       "      <th>...</th>\n",
       "      <th>surface</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind</th>\n",
       "      <th>game_type</th>\n",
       "      <th>weekday</th>\n",
       "      <th>gametime</th>\n",
       "      <th>location</th>\n",
       "      <th>home_coach</th>\n",
       "      <th>away_coach</th>\n",
       "      <th>referee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014_01_GB_SEA</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>9/4/2014</td>\n",
       "      <td>SEA</td>\n",
       "      <td>GB</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>SEA</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>fieldturf</td>\n",
       "      <td>71.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>REG</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>20:30</td>\n",
       "      <td>Home</td>\n",
       "      <td>Pete Carroll</td>\n",
       "      <td>Mike McCarthy</td>\n",
       "      <td>John Parry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014_01_NO_ATL</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>9/7/2014</td>\n",
       "      <td>ATL</td>\n",
       "      <td>NO</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>ATL</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>fieldturf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REG</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Home</td>\n",
       "      <td>Mike Smith</td>\n",
       "      <td>Sean Payton</td>\n",
       "      <td>Bill Leavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014_01_CIN_BAL</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>9/7/2014</td>\n",
       "      <td>BAL</td>\n",
       "      <td>CIN</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>CIN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>sportturf</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>REG</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Home</td>\n",
       "      <td>John Harbaugh</td>\n",
       "      <td>Marvin Lewis</td>\n",
       "      <td>Gene Stetatore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014_01_BUF_CHI</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>9/7/2014</td>\n",
       "      <td>CHI</td>\n",
       "      <td>BUF</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>BUF</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>grass</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>REG</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Home</td>\n",
       "      <td>Marc Trestman</td>\n",
       "      <td>Doug Marrone</td>\n",
       "      <td>Brad Allen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014_01_WAS_HOU</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>9/7/2014</td>\n",
       "      <td>HOU</td>\n",
       "      <td>WAS</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>HOU</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>grass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REG</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Home</td>\n",
       "      <td>Bill O'Brien</td>\n",
       "      <td>Jay Gruden</td>\n",
       "      <td>Jerome Boger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           game_id  season  week      date home_team away_team  home_score  \\\n",
       "0   2014_01_GB_SEA    2014     1  9/4/2014       SEA        GB          36   \n",
       "1   2014_01_NO_ATL    2014     1  9/7/2014       ATL        NO          37   \n",
       "2  2014_01_CIN_BAL    2014     1  9/7/2014       BAL       CIN          16   \n",
       "3  2014_01_BUF_CHI    2014     1  9/7/2014       CHI       BUF          20   \n",
       "4  2014_01_WAS_HOU    2014     1  9/7/2014       HOU       WAS          17   \n",
       "\n",
       "   away_score Winner  home_pass_cmp  ...    surface  temp  wind  game_type  \\\n",
       "0          16    SEA           19.0  ...  fieldturf  71.0  11.0        REG   \n",
       "1          34    ATL           31.0  ...  fieldturf   NaN   NaN        REG   \n",
       "2          23    CIN           35.0  ...  sportturf  74.0   8.0        REG   \n",
       "3          23    BUF           34.0  ...      grass  74.0   3.0        REG   \n",
       "4           6    HOU           14.0  ...      grass   NaN   NaN        REG   \n",
       "\n",
       "    weekday  gametime  location     home_coach     away_coach         referee  \n",
       "0  Thursday     20:30      Home   Pete Carroll  Mike McCarthy      John Parry  \n",
       "1    Sunday     13:00      Home     Mike Smith    Sean Payton      Bill Leavy  \n",
       "2    Sunday     13:00      Home  John Harbaugh   Marvin Lewis  Gene Stetatore  \n",
       "3    Sunday     13:00      Home  Marc Trestman   Doug Marrone      Brad Allen  \n",
       "4    Sunday     13:00      Home   Bill O'Brien     Jay Gruden    Jerome Boger  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 247,
=======
     "execution_count": 47,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"../raw/schedules_raw.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74d256",
   "metadata": {},
   "source": [
    "Add injury report CSV to raw data (source: nflverse.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5b86d17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "pa.Table requires 'pyarrow' module to be installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m seasons = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m2014\u001b[39m, \u001b[32m2025\u001b[39m))\n\u001b[32m     35\u001b[39m output_file = \u001b[33m\"\u001b[39m\u001b[33m../raw/injuries.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mexport_injuries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseasons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mexport_injuries\u001b[39m\u001b[34m(seasons, output_path_csv)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# nflreadpy uses Polars DataFrame by default\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Convert to pandas if you prefer, or write using polars\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     df = \u001b[43minjuries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# If injuries is already pandas, just use it\u001b[39;00m\n\u001b[32m     24\u001b[39m     df = injuries\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sportsbet/lib/python3.11/site-packages/polars/dataframe/frame.py:2561\u001b[39m, in \u001b[36mDataFrame.to_pandas\u001b[39m\u001b[34m(self, use_pyarrow_extension_array, **kwargs)\u001b[39m\n\u001b[32m   2556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Object \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtypes:\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._to_pandas_with_object_columns(\n\u001b[32m   2558\u001b[39m         use_pyarrow_extension_array=use_pyarrow_extension_array, **kwargs\n\u001b[32m   2559\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_pandas_without_object_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_pyarrow_extension_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pyarrow_extension_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   2563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sportsbet/lib/python3.11/site-packages/polars/dataframe/frame.py:2613\u001b[39m, in \u001b[36mDataFrame._to_pandas_without_object_columns\u001b[39m\u001b[34m(self, df, use_pyarrow_extension_array, **kwargs)\u001b[39m\n\u001b[32m   2610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame()\n\u001b[32m   2612\u001b[39m record_batches = df._df.to_pandas()\n\u001b[32m-> \u001b[39m\u001b[32m2613\u001b[39m tbl = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m.from_batches(record_batches)\n\u001b[32m   2614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pyarrow_extension_array:\n\u001b[32m   2615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tbl.to_pandas(\n\u001b[32m   2616\u001b[39m         self_destruct=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2617\u001b[39m         split_blocks=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2618\u001b[39m         types_mapper=\u001b[38;5;28;01mlambda\u001b[39;00m pa_dtype: pd.ArrowDtype(pa_dtype),\n\u001b[32m   2619\u001b[39m         **kwargs,\n\u001b[32m   2620\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/sportsbet/lib/python3.11/site-packages/polars/dependencies.py:102\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    100\u001b[39m pfx = \u001b[38;5;28mself\u001b[39m._mod_pfx.get(\u001b[38;5;28mself\u001b[39m._module_name, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpfx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._module_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m module to be installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: pa.Table requires 'pyarrow' module to be installed"
     ]
    }
   ],
   "source": [
    "# First install nflreadpy if not already installed\n",
    "# pip install nflreadpy\n",
    "\n",
    "import nflreadpy as nfl\n",
    "import os\n",
    "\n",
    "def export_injuries(seasons, output_path_csv):\n",
    "    \"\"\"\n",
    "    Load injury reports for given seasons and save as CSV.\n",
    "\n",
    "    Args:\n",
    "        seasons (list of int): e.g. list(range(2014, 2025))\n",
    "        output_path_csv (str): path to write CSV file, e.g. \"injuries_2014_2024.csv\"\n",
    "    \"\"\"\n",
    "    # Load injury reports for given seasons\n",
    "    injuries = nfl.load_injuries(seasons=seasons)\n",
    "\n",
    "    # Optionally, ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path_csv), exist_ok=True)\n",
    "\n",
    "    # Write out\n",
    "    injuries.write_csv(output_path_csv)\n",
    "    print(f\"Saved injuries data for seasons {seasons} to {output_path_csv}\")\n",
    "\n",
    "# define seasons 2014-2024 inclusive\n",
    "seasons = list(range(2014, 2025))\n",
    "output_file = \"../raw/injuries.csv\"\n",
    "export_injuries(seasons, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224c04d",
   "metadata": {},
   "source": [
    "Calculate metrics using scheduling data\n",
    "1. Have 2 rows per game, one representing each team\n",
    "2. Replace game data with previous week's data - model will only have access to previous week's data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 248,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "592d1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def to_team_games(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # clean any stray whitespace in headers\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # columns we must NOT rename\n",
    "    special_keep = {\"home_team\",\"away_team\",\"home_score\",\"away_score\"}\n",
    "\n",
    "    # detect prefixed columns\n",
    "    home_cols_all = [c for c in df.columns if c.startswith(\"home_\")]\n",
    "    away_cols_all = [c for c in df.columns if c.startswith(\"away_\")]\n",
    "\n",
    "    # stats to rename (exclude team name/score)\n",
    "    home_stats = [c for c in home_cols_all if c not in special_keep]\n",
    "    away_stats = [c for c in away_cols_all if c not in special_keep]\n",
    "\n",
    "    # everything else = base/meta (date, lines, stadium, roof, refs, etc.)\n",
    "    base_cols = [c for c in df.columns if c not in (home_stats + away_stats)]\n",
    "\n",
    "    # helpers\n",
    "    def swap_prefix(cols, old, new):\n",
    "        return {c: c.replace(old, new, 1) for c in cols}\n",
    "\n",
    "    # HOME perspective\n",
    "    home_side = (\n",
    "        df[base_cols + home_stats + away_stats]\n",
    "        .rename(columns={\n",
    "            **swap_prefix(home_stats, \"home_\", \"team_\"),\n",
    "            **swap_prefix(away_stats, \"away_\", \"opp_\"),\n",
    "        })\n",
    "        .assign(\n",
    "            team=lambda d: d[\"home_team\"],\n",
    "            opponent=lambda d: d[\"away_team\"],\n",
    "            team_score=lambda d: d[\"home_score\"],\n",
    "            opp_score=lambda d: d[\"away_score\"],\n",
    "            is_home=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # AWAY perspective\n",
    "    away_side = (\n",
    "        df[base_cols + home_stats + away_stats]\n",
    "        .rename(columns={\n",
    "            **swap_prefix(away_stats, \"away_\", \"team_\"),\n",
    "            **swap_prefix(home_stats, \"home_\", \"opp_\"),\n",
    "        })\n",
    "        .assign(\n",
    "            team=lambda d: d[\"away_team\"],\n",
    "            opponent=lambda d: d[\"home_team\"],\n",
    "            team_score=lambda d: d[\"away_score\"],\n",
    "            opp_score=lambda d: d[\"home_score\"],\n",
    "            is_home=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    team_games = pd.concat([home_side, away_side], ignore_index=True)\n",
    "\n",
    "    # convenient targets\n",
    "    team_games[\"team_win\"]  = (team_games[\"team_score\"] > team_games[\"opp_score\"]).astype(int)\n",
    "    team_games[\"point_diff\"] = team_games[\"team_score\"] - team_games[\"opp_score\"]\n",
    "\n",
    "    # order columns: core â†’ meta â†’ team_* â†’ opp_*\n",
    "    core = [\"game_id\",\"season\",\"week\",\"date\",\"team\",\"opponent\",\"is_home\",\n",
    "            \"team_score\",\"opp_score\",\"team_win\",\"point_diff\"]\n",
    "    meta = [c for c in base_cols if c not in core]\n",
    "    team_stats = sorted([c for c in team_games.columns if c.startswith(\"team_\")])\n",
    "    opp_stats  = sorted([c for c in team_games.columns if c.startswith(\"opp_\")])\n",
    "\n",
    "    ordered = [c for c in core if c in team_games.columns] + meta + team_stats + opp_stats\n",
    "    return team_games[ordered].sort_values([\"season\",\"week\",\"team\"]).reset_index(drop=True)\n",
    "\n",
    "# usage:\n",
    "# df = pd.read_csv(\"../raw/schedules_raw.csv\")\n",
    "df = to_team_games(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8396bc0",
   "metadata": {},
   "source": [
    "Roll back features to ensure model only has access to past data, adding prior_stats columns as necessary"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 249,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "2f304331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prior_features(\n",
    "    team_games: pd.DataFrame,\n",
    "    cols=None,\n",
    "    group_keys=(\"team\",\"season\"),\n",
    "    order_keys=(\"season\",\"week\",\"date\"),\n",
    "    lags=(1,),\n",
    "    name_style=\"auto\",\n",
    "    fill=None\n",
    "):\n",
    "    g = team_games.copy()\n",
    "\n",
    "    # ensure proper sort for shifting\n",
    "    g[\"week\"] = pd.to_numeric(g[\"week\"], errors=\"ignore\")\n",
    "    g = g.sort_values(list(group_keys) + list(order_keys))\n",
    "\n",
    "    # default: all numeric team_* columns\n",
    "    if cols is None:\n",
    "        cols = [c for c in g.columns if c.startswith(\"team_\") and pd.api.types.is_numeric_dtype(g[c])]\n",
    "\n",
    "    def prior_name(col, lag):\n",
    "        if name_style == \"suffix\":\n",
    "            return f\"{col}_prior{lag}\"\n",
    "        if col.startswith(\"team_\"):\n",
    "            return col.replace(\"team_\", f\"team_prior{'' if lag==1 else f'{lag}_'}\", 1)\n",
    "        return f\"{col}_prior{lag}\"\n",
    "\n",
    "    # compute lags per team-season\n",
    "    for lag in lags:\n",
    "        lagged = (\n",
    "            g.groupby(list(group_keys), group_keys=False)[cols]\n",
    "             .shift(lag)\n",
    "             .rename(columns={c: prior_name(c, lag) for c in cols})\n",
    "        )\n",
    "        g = pd.concat([g, lagged], axis=1)\n",
    "\n",
    "    # optional fill for first game(s) of season\n",
    "    if fill is not None:\n",
    "        new_cols = [prior_name(c, lag) for c in cols for lag in lags]\n",
    "        if fill == \"ffill\":\n",
    "            g[new_cols] = (\n",
    "                g.groupby(list(group_keys), group_keys=False)[new_cols]\n",
    "                 .apply(lambda x: x.ffill())\n",
    "            )\n",
    "        else:\n",
    "            g[new_cols] = g[new_cols].fillna(fill)\n",
    "\n",
    "    # --- NEW: drop duplicate columns ---\n",
    "    g = g.loc[:, ~g.columns.duplicated()]\n",
    "\n",
    "    return g\n",
    "\n",
    "# ---- Team stats to lag ----\n",
    "team_cols_to_lag = [\n",
    "    \"team_pass_att\",\n",
    "    \"team_pass_cmp\",\n",
    "    \"team_pass_yds\",\n",
    "    \"team_pass_td\",\n",
    "    \"team_pass_int\",\n",
    "    \"team_pass_sacked\",\n",
    "    \"team_pass_sacked_yds\",\n",
    "    \n",
    "    \"team_rush_att\",\n",
    "    \"team_rush_yds\",\n",
    "    \"team_rush_td\",\n",
    "    \n",
    "    \"team_first_down\",\n",
    "    \"team_turnovers\",\n",
    "    \n",
    "    \"team_penalties\",\n",
    "    \"team_penalties_yds\",\n",
    "    \n",
    "    \"team_fga\", \"team_fgm\",       # field goals\n",
    "    \"team_xpa\", \"team_xpm\",       # extra points\n",
    "    \n",
    "    \"team_punt\",\n",
    "    \"team_punt_yds\",\n",
    "    \n",
    "    \"team_plays_offense\",\n",
    "    \"team_score\"\n",
    "]\n",
    "\n",
    "# ---- Opponent stats to lag ----\n",
    "opp_cols_to_lag = [\n",
    "    \"opp_pass_att\",\n",
    "    \"opp_pass_cmp\",\n",
    "    \"opp_pass_yds\",\n",
    "    \"opp_pass_td\",\n",
    "    \"opp_pass_int\",\n",
    "    \"opp_pass_sacked\",\n",
    "    \"opp_pass_sacked_yds\",\n",
    "    \n",
    "    \"opp_rush_att\",\n",
    "    \"opp_rush_yds\",\n",
    "    \"opp_rush_td\",\n",
    "    \n",
    "    \"opp_first_down\",\n",
    "    \"opp_turnovers\",\n",
    "    \n",
    "    \"opp_penalties\",\n",
    "    \"opp_penalties_yds\",\n",
    "    \n",
    "    \"opp_fga\", \"opp_fgm\",\n",
    "    \"opp_xpa\", \"opp_xpm\",\n",
    "    \n",
    "    \"opp_punt\",\n",
    "    \"opp_punt_yds\",\n",
    "    \n",
    "    \"opp_plays_offense\",\n",
    "    \"opp_score\"\n",
    "]\n",
    "\n",
    "# Team priors\n",
    "df = add_prior_features(\n",
    "    df,\n",
    "    cols=team_cols_to_lag,\n",
    "    group_keys=(\"team\",\"season\"),\n",
    "    name_style=\"suffix\"\n",
    ")\n",
    "\n",
    "# Opponent priors\n",
    "df = add_prior_features(\n",
    "    df,\n",
    "    cols=opp_cols_to_lag,\n",
    "    group_keys=(\"opponent\",\"season\"),\n",
    "    name_style=\"suffix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "24b1c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:30: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:35: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:56: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:61: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_ewm\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = safe_div(df[a], df[b])\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[out] = df[tcol] - df[ocol]\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:139: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"point_diff_roll{w}\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:139: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"point_diff_roll{w}\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:139: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"point_diff_roll{w}\"] = (\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1176785295.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"point_diff_season\"] = np.where(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: rolling averages for selected features\n",
    "team_roll = [\n",
    "    \"team_pass_yds\", \"team_rush_yds\", \"team_pass_td\",\n",
    "    \"team_rush_td\", \"team_turnovers\", \"team_score\",\n",
    "    # extras\n",
    "    \"team_pass_att\", \"team_rush_att\",\n",
    "    \"team_first_down\", \"team_penalties\",\n",
    "    \"team_plays_offense\",\n",
    "    \"team_pass_sacked\",\n",
    "    \"team_fga\"\n",
    "]\n",
    "\n",
    "opp_roll = [\n",
    "    \"opp_pass_yds\", \"opp_rush_yds\", \"opp_pass_td\",\n",
    "    \"opp_rush_td\", \"opp_turnovers\", \"opp_score\",\n",
    "    # extras\n",
    "    \"opp_pass_att\", \"opp_rush_att\",\n",
    "    \"opp_first_down\", \"opp_penalties\",\n",
    "    \"opp_plays_offense\",\n",
    "    \"opp_pass_sacked\"\n",
    "]\n",
    "\n",
    "# --- roll3 / roll5 / roll10 ---\n",
    "for w in (3, 5, 10):\n",
    "    for col in team_roll:\n",
    "        df[f\"{col}_roll{w}\"] = (\n",
    "            df.groupby(\"team\")[col]\n",
    "              .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "    for col in opp_roll:\n",
    "        df[f\"{col}_roll{w}\"] = (\n",
    "            df.groupby(\"opponent\")[col]\n",
    "              .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "\n",
    "# --- season-to-date mean (before current game) ---\n",
    "for col in team_roll:\n",
    "    grp = df.groupby([\"team\",\"season\"], sort=False)[col]\n",
    "    csum = grp.cumsum().shift(1)\n",
    "    cnt  = grp.cumcount()\n",
    "    df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
    "\n",
    "for col in opp_roll:\n",
    "    grp = df.groupby([\"opponent\",\"season\"], sort=False)[col]\n",
    "    csum = grp.cumsum().shift(1)\n",
    "    cnt  = grp.cumcount()\n",
    "    df[f\"{col}_season\"] = np.where(cnt > 0, csum / cnt, np.nan)\n",
    "\n",
    "# --- exponentially weighted momentum ---\n",
    "alpha = 0.3\n",
    "for col in team_roll:\n",
    "    df[f\"{col}_ewm\"] = (\n",
    "        df.groupby(\"team\")[col]\n",
    "          .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
    "    )\n",
    "for col in opp_roll:\n",
    "    df[f\"{col}_ewm\"] = (\n",
    "        df.groupby(\"opponent\")[col]\n",
    "          .apply(lambda x: x.shift(1).ewm(alpha=alpha, adjust=False, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "# ---------- Safe division helper ----------\n",
    "def safe_div(num, den):\n",
    "    return np.where(den.astype(float) != 0, num.astype(float) / den.astype(float), np.nan)\n",
    "\n",
    "# ---------- Windows to compute ----------\n",
    "windows = [\"roll3\", \"roll5\", \"roll10\", \"season\"]\n",
    "\n",
    "def rate_if_exists(lhs_base, rhs_base, out_base, windows):\n",
    "    for w in windows:\n",
    "        a, b, out = f\"{lhs_base}_{w}\", f\"{rhs_base}_{w}\", f\"{out_base}_{w}\"\n",
    "        if a in df.columns and b in df.columns:\n",
    "            df[out] = safe_div(df[a], df[b])\n",
    "\n",
    "# ---------- Team + Opp Efficiency Rates ----------\n",
    "# Yards/attempt\n",
    "rate_if_exists(\"team_pass_yds\", \"team_pass_att\", \"team_pass_ypa\", windows)\n",
    "rate_if_exists(\"opp_pass_yds\",  \"opp_pass_att\",  \"opp_pass_ypa\",  windows)\n",
    "\n",
    "# TD rate\n",
    "rate_if_exists(\"team_pass_td\", \"team_pass_att\", \"team_pass_td_rate\", windows)\n",
    "rate_if_exists(\"opp_pass_td\",  \"opp_pass_att\",  \"opp_pass_td_rate\", windows)\n",
    "\n",
    "# INT rate\n",
    "rate_if_exists(\"team_pass_int\", \"team_pass_att\", \"team_int_rate\", windows)\n",
    "rate_if_exists(\"opp_pass_int\",  \"opp_pass_att\",  \"opp_int_rate\",  windows)\n",
    "\n",
    "# Rush yards/carry\n",
    "rate_if_exists(\"team_rush_yds\", \"team_rush_att\", \"team_rush_ypc\", windows)\n",
    "rate_if_exists(\"opp_rush_yds\",  \"opp_rush_att\",  \"opp_rush_ypc\",  windows)\n",
    "\n",
    "# Play mix\n",
    "rate_if_exists(\"team_pass_att\", \"team_plays_offense\", \"team_pass_rate\", windows)\n",
    "rate_if_exists(\"opp_pass_att\",  \"opp_plays_offense\",  \"opp_pass_rate\",  windows)\n",
    "rate_if_exists(\"team_rush_att\", \"team_plays_offense\", \"team_rush_rate\", windows)\n",
    "rate_if_exists(\"opp_rush_att\",  \"opp_plays_offense\",  \"opp_rush_rate\",  windows)\n",
    "\n",
    "# First-down rate\n",
    "rate_if_exists(\"team_first_down\", \"team_plays_offense\", \"team_fd_rate\", windows)\n",
    "rate_if_exists(\"opp_first_down\",  \"opp_plays_offense\",  \"opp_fd_rate\",  windows)\n",
    "\n",
    "# FG attempt rate\n",
    "rate_if_exists(\"team_fga\", \"team_plays_offense\", \"team_fga_rate\", windows)\n",
    "rate_if_exists(\"opp_fga\",  \"opp_plays_offense\",  \"opp_fga_rate\",  windows)\n",
    "\n",
    "# ---------- Differentials (matchup framing) ----------\n",
    "# Raw stats (example subset)\n",
    "raw_pairs = [\n",
    "    \"pass_yds\",\"rush_yds\",\"pass_td\",\"rush_td\",\"turnovers\",\"score\",\"first_down\"\n",
    "]\n",
    "for stat in raw_pairs:\n",
    "    for w in windows:\n",
    "        tcol, ocol, out = f\"team_{stat}_{w}\", f\"opp_{stat}_{w}\", f\"diff_{stat}_{w}\"\n",
    "        if tcol in df.columns and ocol in df.columns:\n",
    "            df[out] = df[tcol] - df[ocol]\n",
    "\n",
    "# Efficiency differentials\n",
    "eff_pairs = [\"pass_ypa\",\"rush_ypc\",\"pass_td_rate\",\"int_rate\",\"fd_rate\",\"fga_rate\"]\n",
    "for stat in eff_pairs:\n",
    "    for w in windows:\n",
    "        tcol, ocol, out = f\"team_{stat}_{w}\", f\"opp_{stat}_{w}\", f\"diff_{stat}_{w}\"\n",
    "        if tcol in df.columns and ocol in df.columns:\n",
    "            df[out] = df[tcol] - df[ocol]\n",
    "\n",
    "# Mix differentials\n",
    "for stat in [\"pass_rate\",\"rush_rate\"]:\n",
    "    for w in windows:\n",
    "        tcol, ocol, out = f\"team_{stat}_{w}\", f\"opp_{stat}_{w}\", f\"diff_{stat}_{w}\"\n",
    "        if tcol in df.columns and ocol in df.columns:\n",
    "            df[out] = df[tcol] - df[ocol]\n",
    "\n",
    "# ---------- Point Differential windows ----------\n",
    "if \"point_diff\" in df.columns:\n",
    "    for w in (3, 5, 10):\n",
    "        df[f\"point_diff_roll{w}\"] = (\n",
    "            df.groupby(\"team\")[\"point_diff\"]\n",
    "              .apply(lambda x: x.shift(1).rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "    g = df.groupby([\"team\",\"season\"], sort=False)[\"point_diff\"]\n",
    "    df[\"point_diff_season\"] = np.where(\n",
    "        g.cumcount() > 0, g.cumsum().shift(1) / g.cumcount(), np.nan\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 251,
   "id": "df195555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"team_win_pre\"]   = df.groupby(\"team\")[\"team_win\"].shift(1).cumsum()\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:15: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:15: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:15: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:19: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:28: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:28: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:28: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2969453775.py:43: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n"
     ]
    }
   ],
   "source": [
    "# 1) Opponent win % BEFORE this game\n",
    "df[\"team_win_pre\"]   = df.groupby(\"team\")[\"team_win\"].shift(1).cumsum()\n",
    "df[\"team_games_pre\"] = df.groupby(\"team\").cumcount()\n",
    "df[\"team_win_pct_pre\"] = np.where(\n",
    "    df[\"team_games_pre\"] > 0, df[\"team_win_pre\"] / df[\"team_games_pre\"], np.nan\n",
    ")\n",
    "\n",
    "# opponent's win% prior to this game (must be lagged!)\n",
    "df[\"opponent_win_pct_pre\"] = df.groupby(\"opponent\")[\"team_win_pct_pre\"].shift(1)\n",
    "\n",
    "# 2) SoS: mean opponent win% over last N games (team & opp mirrors)\n",
    "for w in (3, 5, 10):\n",
    "    df[f\"team_sos_win_pct_roll{w}\"] = (\n",
    "        df.groupby(\"team\")[\"opponent_win_pct_pre\"]\n",
    "          .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "    )\n",
    "    df[f\"opp_sos_win_pct_roll{w}\"] = (\n",
    "        df.groupby(\"opponent\")[\"team_win_pct_pre\"]\n",
    "          .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "# 3) Team rolling point diff (if not already built elsewhere)\n",
    "for w in (3, 5, 10):\n",
    "    col = f\"team_point_diff_roll{w}\"\n",
    "    if col not in df.columns:\n",
    "        df[col] = (\n",
    "            df.groupby(\"team\")[\"point_diff\"]\n",
    "              .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "\n",
    "# 4) League-average point diff per week, then lagged rolling by SEASON\n",
    "#    (avoids same-week peeking)\n",
    "league_week = (\n",
    "    df.groupby([\"season\",\"week\"], as_index=False)[\"point_diff\"]\n",
    "      .mean()\n",
    "      .rename(columns={\"point_diff\": \"league_avg_pd\"})\n",
    "      .sort_values([\"season\",\"week\"])\n",
    ")\n",
    "\n",
    "for w in (3, 5, 10):\n",
    "    league_week[f\"league_point_diff_roll{w}\"] = (\n",
    "        league_week.groupby(\"season\")[\"league_avg_pd\"]\n",
    "                   .apply(lambda s: s.shift(1).rolling(w, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "# attach back\n",
    "df = df.merge(league_week, on=[\"season\",\"week\"], how=\"left\")\n",
    "\n",
    "# 5) Adjusted PD = team rolling PD âˆ’ league rolling PD (same window)\n",
    "for w in (3, 5, 10):\n",
    "    df[f\"team_adj_point_diff_roll{w}\"] = (\n",
    "        df[f\"team_point_diff_roll{w}\"] - df[f\"league_point_diff_roll{w}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f74d4f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:14: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df.groupby(\"team\")[\"team_win\"].apply(lambda x: calc_streak(x.shift(1).fillna(0), 1))\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:17: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df.groupby(\"team\")[\"team_win\"].apply(lambda x: calc_streak(x.shift(1).fillna(1), 0))\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:25: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:31: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:25: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:31: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:25: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\1339401465.py:31: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n"
     ]
    }
   ],
   "source": [
    "# --- Win & loss streaks ---\n",
    "def calc_streak(series, win_val=1):\n",
    "    streak = []\n",
    "    count = 0\n",
    "    for val in series:\n",
    "        if val == win_val:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        streak.append(count)\n",
    "    return pd.Series(streak, index=series.index)\n",
    "\n",
    "df[\"team_win_streak_pre\"] = (\n",
    "    df.groupby(\"team\")[\"team_win\"].apply(lambda x: calc_streak(x.shift(1).fillna(0), 1))\n",
    ")\n",
    "df[\"team_loss_streak_pre\"] = (\n",
    "    df.groupby(\"team\")[\"team_win\"].apply(lambda x: calc_streak(x.shift(1).fillna(1), 0))\n",
    ")\n",
    "\n",
    "# --- Blowout & close-game rates ---\n",
    "for w in (3, 5, 10):\n",
    "    # blowouts: |PD| >= 14\n",
    "    df[f\"blowout_rate_roll{w}\"] = (\n",
    "        df.groupby(\"team\")[\"point_diff\"]\n",
    "          .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
    "              lambda s: np.mean(np.abs(s) >= 14)))\n",
    "    )\n",
    "    # close games: |PD| <= 3\n",
    "    df[f\"close_game_rate_roll{w}\"] = (\n",
    "        df.groupby(\"team\")[\"point_diff\"]\n",
    "          .apply(lambda x: x.shift(1).rolling(w, min_periods=1).apply(\n",
    "              lambda s: np.mean(np.abs(s) <= 3)))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1ad7ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) HOME / VENUE / WEATHER\n",
    "# =========================\n",
    "\n",
    "def to_flag(s):  # -> 0/1 Int8, NA -> 0\n",
    "    return s.fillna(False).astype(\"Int8\")\n",
    "\n",
    "if \"temp\" in df.columns:\n",
    "    df[\"temp_bin_le_32\"] = to_flag(df[\"temp\"] <= 32)\n",
    "    df[\"temp_bin_33_60\"] = to_flag((df[\"temp\"] > 32) & (df[\"temp\"] <= 60))\n",
    "    df[\"temp_bin_61_80\"] = to_flag((df[\"temp\"] > 60) & (df[\"temp\"] <= 80))\n",
    "    df[\"temp_bin_gt_80\"] = to_flag(df[\"temp\"] > 80)\n",
    "    df[\"extreme_cold\"]   = to_flag(df[\"temp\"] <= 32)\n",
    "\n",
    "if \"wind\" in df.columns:\n",
    "    df[\"wind2\"] = df[\"wind\"].astype(float) ** 2  # non-linear wind effect\n",
    "\n",
    "if {\"is_home\",\"temp\"}.issubset(df.columns):\n",
    "    # leave float to allow NA in temp\n",
    "    df[\"is_home_x_temp\"] = df[\"is_home\"].astype(float) * df[\"temp\"]\n",
    "\n",
    "if {\"is_home\",\"short_rest\"}.issubset(df.columns):\n",
    "    df[\"is_home_x_short_rest\"] = to_flag(df[\"is_home\"].astype(bool) & df[\"short_rest\"].astype(bool))\n",
    "\n",
    "if \"roof\" in df.columns:\n",
    "    roof_l = df[\"roof\"].astype(\"string\").str.lower()\n",
    "    df[\"roof_dome\"] = to_flag(roof_l.isin([\"dome\",\"indoor\",\"closed\"]))\n",
    "\n",
    "if \"surface\" in df.columns:\n",
    "    surf_l = df[\"surface\"].astype(\"string\").str.lower()\n",
    "    df[\"surface_turf\"] = to_flag(surf_l.str.contains(\"turf|artificial\", na=False, regex=True))\n",
    "\n",
    "# =================================\n",
    "# 8) SEASON CONTEXT & GAME TIMING\n",
    "# =================================\n",
    "\n",
    "if \"week\" in df.columns:\n",
    "    df[\"week_number\"] = df[\"week\"].astype(float)\n",
    "    df[\"week_scaled\"] = df[\"week_number\"] / 18.0\n",
    "    df[\"late_season\"] = to_flag(df[\"week_number\"] >= 14)\n",
    "\n",
    "if {\"team\",\"season\",\"week\",\"team_win\"}.issubset(df.columns):\n",
    "    df[\"team_win_pre\"]   = df.groupby(\"team\")[\"team_win\"].shift(1).cumsum()\n",
    "    df[\"team_games_pre\"] = df.groupby(\"team\").cumcount()\n",
    "    df[\"team_wins_pre\"]  = df[\"team_win_pre\"].fillna(0).astype(int)\n",
    "\n",
    "    if \"late_season\" in df.columns:\n",
    "        df[\"must_win_proxy\"] = to_flag((df[\"team_wins_pre\"] >= 8) & (df[\"late_season\"] == 1))\n",
    "\n",
    "# Divisional flag if you have division info\n",
    "if {\"team_division\",\"opponent_division\"}.issubset(df.columns):\n",
    "    df[\"is_divisional\"] = to_flag(df[\"team_division\"] == df[\"opponent_division\"])\n",
    "\n",
    "# Travel/timezone proxies if available\n",
    "if {\"team_tz\",\"game_tz\"}.issubset(df.columns):\n",
    "    df[\"west_to_east\"] = to_flag(df[\"team_tz\"] > df[\"game_tz\"])\n",
    "    df[\"east_to_west\"] = to_flag(df[\"team_tz\"] < df[\"game_tz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "08654e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:49: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  mu  = grp.apply(lambda s: s.expanding(min_periods=1).mean().shift(1)).reset_index(level=0, drop=True)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:50: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  sd  = grp.apply(lambda s: s.expanding(min_periods=2).std(ddof=0).shift(1)).reset_index(level=0, drop=True)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:49: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  mu  = grp.apply(lambda s: s.expanding(min_periods=1).mean().shift(1)).reset_index(level=0, drop=True)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:50: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  sd  = grp.apply(lambda s: s.expanding(min_periods=2).std(ddof=0).shift(1)).reset_index(level=0, drop=True)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:49: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  mu  = grp.apply(lambda s: s.expanding(min_periods=1).mean().shift(1)).reset_index(level=0, drop=True)\n",
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\2509293458.py:50: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  sd  = grp.apply(lambda s: s.expanding(min_periods=2).std(ddof=0).shift(1)).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def safe_div(a, b):\n",
    "    a = a.astype(float)\n",
    "    b = b.astype(float)\n",
    "    return np.where(b != 0, a / b, np.nan)\n",
    "\n",
    "# =========================\n",
    "# 10) SPECIAL TEAMS PROXIES\n",
    "# =========================\n",
    "# FG per-play rate + differentials (uses your existing roll windows)\n",
    "for w in (3, 5, 10):\n",
    "    fga_t   = f\"team_fga_roll{w}\"\n",
    "    plays_t = f\"team_plays_offense_roll{w}\"\n",
    "    fga_o   = f\"opp_fga_roll{w}\"\n",
    "    plays_o = f\"opp_plays_offense_roll{w}\"\n",
    "\n",
    "    if fga_t in df.columns and plays_t in df.columns:\n",
    "        df[f\"team_fg_per_play_rate_roll{w}\"] = safe_div(df[fga_t], df[plays_t])\n",
    "\n",
    "    if fga_o in df.columns and plays_o in df.columns:\n",
    "        df[f\"opp_fg_per_play_rate_roll{w}\"]  = safe_div(df[fga_o], df[plays_o])\n",
    "\n",
    "    tcol = f\"team_fg_per_play_rate_roll{w}\"\n",
    "    ocol = f\"opp_fg_per_play_rate_roll{w}\"\n",
    "    if tcol in df.columns and ocol in df.columns:\n",
    "        df[f\"diff_fg_per_play_rate_roll{w}\"] = df[tcol] - df[ocol]\n",
    "\n",
    "# (Optional FG \"make opportunity\" = attempts per game-in-window)\n",
    "# If you want it later: team_fga_roll{w} / w\n",
    "\n",
    "# ================================================\n",
    "# 11) SEASON-TO-DATE Z-SCORES (normalize in-season)\n",
    "# ================================================\n",
    "# Weâ€™ll z-score the season-to-date versions you already created:\n",
    "#   team_pass_ypa_season, team_rush_ypc_season, team_fd_rate_season\n",
    "# Mean/STD are computed per season using only PRIOR weeks (shifted).\n",
    "\n",
    "season_stats = [\"team_pass_ypa\", \"team_rush_ypc\", \"team_fd_rate\"]\n",
    "\n",
    "for base in season_stats:\n",
    "    col = f\"{base}_season\"\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    # per-season expanding mean/std, both lagged so week N uses data up to N-1\n",
    "    grp = df.groupby(\"season\")[col]\n",
    "    mu  = grp.apply(lambda s: s.expanding(min_periods=1).mean().shift(1)).reset_index(level=0, drop=True)\n",
    "    sd  = grp.apply(lambda s: s.expanding(min_periods=2).std(ddof=0).shift(1)).reset_index(level=0, drop=True)\n",
    "\n",
    "    zname = f\"{base}_season_z\"\n",
    "    df[zname] = np.where((sd.astype(float) > 0) & (~pd.isna(sd)),\n",
    "                         (df[col].astype(float) - mu.astype(float)) / sd.astype(float),\n",
    "                         np.nan)\n",
    "\n",
    "# ==================================================\n",
    "# 12) HIGH-VALUE INTERACTIONS (kept deliberately few)\n",
    "# ==================================================\n",
    "# is_home * team_win_pct_pre\n",
    "if \"is_home\" in df.columns and \"team_win_pct_pre\" in df.columns:\n",
    "    df[\"int_is_home__team_win_pct_pre\"] = df[\"is_home\"].astype(float) * df[\"team_win_pct_pre\"].astype(float)\n",
    "\n",
    "# is_home * diff_pass_ypa_roll5\n",
    "if \"is_home\" in df.columns and \"diff_pass_ypa_roll5\" in df.columns:\n",
    "    df[\"int_is_home__diff_pass_ypa_roll5\"] = df[\"is_home\"].astype(float) * df[\"diff_pass_ypa_roll5\"].astype(float)\n",
    "\n",
    "# short_rest * diff_rush_ypc_roll3\n",
    "if \"short_rest\" in df.columns and \"diff_rush_ypc_roll3\" in df.columns:\n",
    "    # keep as float so missing short_rest doesnâ€™t crash\n",
    "    df[\"int_short_rest__diff_rush_ypc_roll3\"] = df[\"short_rest\"].astype(float) * df[\"diff_rush_ypc_roll3\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923acb3",
   "metadata": {},
   "source": [
    "Add point differential metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0eb302ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmart\\AppData\\Local\\Temp\\ipykernel_42232\\283384497.py:9: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda x: x.shift(1).rolling(3, min_periods=1).mean())\n"
     ]
    }
   ],
   "source": [
    "# --- Prior game point differential (1-game lag)\n",
    "df[\"point_diff_prior1\"] = (\n",
    "    df.groupby(\"team\")[\"point_diff\"].shift(1)\n",
    ")\n",
    "\n",
    "# --- Rolling 3-game average point differential\n",
    "df[\"point_diff_roll3\"] = (\n",
    "    df.groupby(\"team\")[\"point_diff\"]\n",
    "      .apply(lambda x: x.shift(1).rolling(3, min_periods=1).mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "f4fc74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute averages\n",
    "\n",
    "#impute probablitiy feature without vig\n",
    "import numpy as np\n",
    "\n",
    "def moneyline_to_prob(ml):\n",
    "    \"\"\"Convert American odds to implied probability (with vig).\"\"\"\n",
    "    if ml < 0:\n",
    "        return -ml / (-ml + 100)\n",
    "    else:\n",
    "        return 100 / (ml + 100)\n",
    "\n",
    "def remove_vig_prob(team_ml, opp_ml):\n",
    "    \"\"\"\n",
    "    Convert team/opponent moneylines into normalized probabilities.\n",
    "    Returns (team_prob, opp_prob) with vig removed.\n",
    "    \"\"\"\n",
    "    p_team = moneyline_to_prob(team_ml)\n",
    "    p_opp = moneyline_to_prob(opp_ml)\n",
    "    total = p_team + p_opp\n",
    "    return p_team / total, p_opp / total\n",
    "\n",
    "# --- Apply to your DataFrame ---\n",
    "df[\"team_prob_novig\"], df[\"opp_prob_novig\"] = zip(\n",
    "    *df[[\"team_moneyline\", \"opp_moneyline\"]].apply(\n",
    "        lambda x: remove_vig_prob(x[\"team_moneyline\"], x[\"opp_moneyline\"]), axis=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43048ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bdd936a",
   "metadata": {},
   "source": [
    "General Cleaning\n",
    "1. Check for low variability columns\n",
    "2. Check for missingness"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 257,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "d0973b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant columns: ['team_pass_sacked_yds', 'team_punt', 'team_punt_yds', 'opp_pass_sacked_yds', 'opp_punt', 'opp_punt_yds', 'team_pass_sacked_yds_prior1', 'team_punt_prior1', 'team_punt_yds_prior1', 'opp_pass_sacked_yds_prior1', 'opp_punt_prior1', 'opp_punt_yds_prior1', 'league_avg_pd', 'league_point_diff_roll3', 'league_point_diff_roll5', 'league_point_diff_roll10']\n",
      "Constant columns: ['league_avg_pd', 'league_point_diff_roll3', 'league_point_diff_roll5', 'league_point_diff_roll10']\n",
      "                      column  missing_count  missing_pct\n",
      "0                      wind2           1802    33.064220\n",
      "1                       wind           1802    33.064220\n",
      "2                       temp           1802    33.064220\n",
      "3             is_home_x_temp           1802    33.064220\n",
      "4      diff_pass_rate_season            928    17.027523\n",
      "5       diff_rush_ypc_season            928    17.027523\n",
      "6       diff_pass_ypa_season            928    17.027523\n",
      "7     diff_first_down_season            928    17.027523\n",
      "8        diff_fd_rate_season            928    17.027523\n",
      "9      diff_turnovers_season            928    17.027523\n",
      "10      diff_rush_yds_season            928    17.027523\n",
      "11  diff_pass_td_rate_season            928    17.027523\n",
      "12       diff_rush_td_season            928    17.027523\n",
      "13      diff_pass_yds_season            928    17.027523\n",
      "14     diff_rush_rate_season            928    17.027523\n",
      "15       diff_pass_td_season            928    17.027523\n",
      "16         diff_score_season            614    11.266055\n",
      "17    team_pass_ypa_season_z            509     9.339450\n",
      "18    team_rush_ypc_season_z            509     9.339450\n",
      "19     team_fd_rate_season_z            509     9.339450\n"
     ]
    }
   ],
   "source": [
    "print(\"Constant columns:\", df.columns[df.nunique(dropna=True) <= 1].tolist())\n",
    "\n",
    "const_cols = [\n",
    "    'team_pass_sacked_yds', 'team_punt', 'team_punt_yds',\n",
    "    'opp_pass_sacked_yds', 'opp_punt', 'opp_punt_yds',\n",
    "    'team_pass_sacked_yds_prior1', 'team_punt_prior1', 'team_punt_yds_prior1',\n",
    "    'opp_pass_sacked_yds_prior1', 'opp_punt_prior1', 'opp_punt_yds_prior1'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=const_cols)\n",
    "\n",
    "print(\"Constant columns:\", df.columns[df.nunique(dropna=True) <= 1].tolist())\n",
    "\n",
    "df = df.sort_values([\"season\", \"week\", \"team\"]).reset_index(drop=True)\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "missing = (\n",
    "    df.isna()\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\": \"column\", 0: \"missing_count\"})\n",
    ")\n",
    "\n",
    "# Add % missing\n",
    "missing[\"missing_pct\"] = (missing[\"missing_count\"] / len(df)) * 100\n",
    "\n",
    "# Sort by % missing\n",
    "missing = missing.sort_values(\"missing_pct\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(missing.head(20))  # Top 20 columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80804063",
   "metadata": {},
   "source": [
    "Adding feature for number of rest days since last game: rest_days"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 258,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "812a2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make sure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort so diffs are correct\n",
    "df = df.sort_values(['team', 'season', 'week', 'date'])\n",
    "\n",
    "# Previous game date and season for each team\n",
    "df['__prev_date'] = df.groupby('team')['date'].shift(1)\n",
    "df['__prev_season'] = df.groupby('team')['season'].shift(1)\n",
    "\n",
    "# Rest days only within the same season (avoid giant off-season gaps)\n",
    "rest_days = (df['date'] - df['__prev_date']).dt.days\n",
    "df['rest_days'] = np.where(df['season'].eq(df['__prev_season']), rest_days, np.nan)\n",
    "\n",
    "# Helpful indicators (set to <NA> when rest_days is NaN)\n",
    "df['short_rest'] = (df['rest_days'] <= 6).astype('Int64')\n",
    "df['bye_week']   = (df['rest_days'] >= 13).astype('Int64')\n",
    "df.loc[df['rest_days'].isna(), ['short_rest', 'bye_week']] = pd.NA\n",
    "\n",
    "# Drop helper columns\n",
    "df.drop(columns=['__prev_date','__prev_season'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "113633d1",
   "metadata": {},
   "source": [
    "Game and Season Record Data"
=======
   "id": "ca9c2c9f",
   "metadata": {},
   "source": [
    "Features to compute the win rate for the last 3 games () and the last 5 games ()"
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 259,
   "id": "9d131d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Chronological order\n",
    "df = df.sort_values([\"team\", \"season\", \"week\"])\n",
    "g  = df.groupby([\"team\", \"season\"], sort=False)\n",
    "\n",
    "# Previous week's win (within-season)\n",
    "df[\"team_win_prev1\"] = g[\"team_win\"].shift(1).fillna(0).astype(int)\n",
    "\n",
    "# Games played BEFORE this game (0 for season opener)\n",
    "df[\"team_games_played_pre\"] = g.cumcount().astype(int)\n",
    "\n",
    "# Wins BEFORE this game: shift THEN cumsum (within-season)\n",
    "df[\"team_wins_pre\"] = g[\"team_win\"].shift(1).fillna(0).astype(int)\n",
    "df[\"team_wins_pre\"] = g[\"team_wins_pre\"].cumsum().astype(int)\n",
    "\n",
    "# Losses BEFORE this game\n",
    "df[\"team_losses_pre\"] = (df[\"team_games_played_pre\"] - df[\"team_wins_pre\"]).astype(int)\n",
    "\n",
    "# Win% BEFORE this game (NaN for opener; fill 0.0 if you prefer)\n",
    "df[\"team_win_pct_pre\"] = np.where(\n",
    "    df[\"team_games_played_pre\"] > 0,\n",
    "    df[\"team_wins_pre\"] / df[\"team_games_played_pre\"],\n",
    "    np.nan\n",
    ").astype(float)"
=======
   "execution_count": null,
   "id": "a3f2bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Engineering: Rolling win rates (3/5) with rollover, start at game #6 per team ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure types & chronological order\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['team_win'] = df['team_win'].astype(int)\n",
    "df = df.sort_values(['team','season','week','date']).reset_index(drop=True)\n",
    "\n",
    "# Per-team game index across the entire dataset (rollover across seasons)\n",
    "df['__team_game_no'] = df.groupby('team').cumcount() + 1\n",
    "\n",
    "# Rollover rolling win rates (ONLY past games via shift(1))\n",
    "g_team = df.groupby('team')\n",
    "# Require full windows to avoid noisy partials; first valid at game #4 for l3 and #6 for l5\n",
    "win_hist = g_team['team_win'].shift(1)   # past-only series\n",
    "\n",
    "df['winrate_l3'] = win_hist.rolling(window=3, min_periods=3).mean()\n",
    "df['winrate_l5'] = win_hist.rolling(window=5, min_periods=5).mean()\n",
    "\n",
    "# Suppress outputs until each team's 6th game in THIS dataset\n",
    "# (This makes the first emitted l3/l5 appear at game #6, using games #1â€“5 as the seed history.)\n",
    "mask_early = df['__team_game_no'] <= 5\n",
    "df.loc[mask_early, ['winrate_l3','winrate_l5']] = np.nan\n",
    "\n",
    "# Optional: if you want to expose the \"first valid game\" explicitly (useful for debugging)\n",
    "# df['winrate_first_valid_game'] = 6\n",
    "\n",
    "# Clean up helper\n",
    "df.drop(columns=['__team_game_no'], inplace=True)\n"
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4aa06",
   "metadata": {},
   "source": [
    "Print to the new intermediate dataset"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 260,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "ea5e8719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           game_id  season  week       date team opponent  is_home  \\\n",
      "0   2014_01_SD_ARI    2014     1 2014-09-08  ARI       SD        1   \n",
      "1  2014_02_ARI_NYG    2014     2 2014-09-14  ARI      NYG        0   \n",
      "2   2014_03_SF_ARI    2014     3 2014-09-21  ARI       SF        1   \n",
      "3  2014_05_ARI_DEN    2014     5 2014-10-05  ARI      DEN        0   \n",
      "4  2014_06_WAS_ARI    2014     6 2014-10-12  ARI      WAS        1   \n",
      "\n",
<<<<<<< HEAD
      "     team_score  opp_score  team_win  ...  int_is_home__diff_pass_ypa_roll5  \\\n",
      "0            18         17         1  ...                               NaN   \n",
      "32           25         14         1  ...                          0.000000   \n",
      "64           23         14         1  ...                         -1.889895   \n",
      "122          20         41         0  ...                         -0.000000   \n",
      "152          30         20         1  ...                         -0.546091   \n",
      "\n",
      "    point_diff_prior1 team_prob_novig  opp_prob_novig  rest_days short_rest  \\\n",
      "0                14.0        0.591679        0.408321        NaN       <NA>   \n",
      "32                8.0        0.478203        0.521797        6.0          1   \n",
      "64               -1.0        0.413907        0.586093        7.0          0   \n",
      "122              12.0        0.240964        0.759036       14.0          0   \n",
      "152              25.0        0.680078        0.319922        7.0          0   \n",
      "\n",
      "     bye_week  team_win_prev1 team_games_played_pre team_losses_pre  \n",
      "0        <NA>               0                     0               0  \n",
      "32          0               1                     1               0  \n",
      "64          0               1                     2               0  \n",
      "122         1               1                     3               0  \n",
      "152         0               0                     4               1  \n",
      "\n",
      "[5 rows x 400 columns]\n"
=======
      "   team_score  opp_score  team_win  ...  opp_xpm_prior1  \\\n",
      "0          18         17         1  ...             NaN   \n",
      "1          25         14         1  ...             2.0   \n",
      "2          23         14         1  ...             2.0   \n",
      "3          20         41         0  ...             1.0   \n",
      "4          30         20         1  ...             2.0   \n",
      "\n",
      "  opp_plays_offense_prior1 opp_score_prior1  team_prob_novig  opp_prob_novig  \\\n",
      "0                      NaN              NaN         0.591679        0.408321   \n",
      "1                     79.0             14.0         0.478203        0.521797   \n",
      "2                     83.0             20.0         0.413907        0.586093   \n",
      "3                     93.0             20.0         0.240964        0.759036   \n",
      "4                     77.0             17.0         0.680078        0.319922   \n",
      "\n",
      "  rest_days  short_rest  bye_week winrate_l3 winrate_l5  \n",
      "0       NaN        <NA>      <NA>        NaN        NaN  \n",
      "1       6.0           1         0        NaN        NaN  \n",
      "2       7.0           0         0        NaN        NaN  \n",
      "3      14.0           0         1        NaN        NaN  \n",
      "4       7.0           0         0        NaN        NaN  \n",
      "\n",
      "[5 rows x 115 columns]\n"
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
     ]
    }
   ],
   "source": [
    "# save\n",
    "df.to_csv(\"../intermediate/schedules_cleaned.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 261,
=======
   "execution_count": null,
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
   "id": "a2c3fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['game_id', 'season', 'week', 'date', 'team', 'opponent', 'is_home', 'team_score', 'opp_score', 'team_win', 'point_diff', 'home_team', 'away_team', 'home_score', 'away_score', 'Winner', 'spread_line', 'total_line', 'stadium', 'roof', 'surface', 'temp', 'wind', 'game_type', 'weekday', 'gametime', 'location', 'referee', 'team_coach', 'team_fga', 'team_fgm', 'team_first_down', 'team_fumbles_lost', 'team_moneyline', 'team_pass_att', 'team_pass_cmp', 'team_pass_int', 'team_pass_sacked', 'team_pass_td', 'team_pass_yds', 'team_penalties', 'team_penalties_yds', 'team_plays_offense', 'team_rush_att', 'team_rush_td', 'team_rush_yds', 'team_turnovers', 'team_xpa', 'team_xpm', 'opp_coach', 'opp_fga', 'opp_fgm', 'opp_first_down', 'opp_fumbles_lost', 'opp_moneyline', 'opp_pass_att', 'opp_pass_cmp', 'opp_pass_int', 'opp_pass_sacked', 'opp_pass_td', 'opp_pass_yds', 'opp_penalties', 'opp_penalties_yds', 'opp_plays_offense', 'opp_rush_att', 'opp_rush_td', 'opp_rush_yds', 'opp_turnovers', 'opp_xpa', 'opp_xpm', 'team_pass_att_prior1', 'team_pass_cmp_prior1', 'team_pass_yds_prior1', 'team_pass_td_prior1', 'team_pass_int_prior1', 'team_pass_sacked_prior1', 'team_rush_att_prior1', 'team_rush_yds_prior1', 'team_rush_td_prior1', 'team_first_down_prior1', 'team_turnovers_prior1', 'team_penalties_prior1', 'team_penalties_yds_prior1', 'team_fga_prior1', 'team_fgm_prior1', 'team_xpa_prior1', 'team_xpm_prior1', 'team_plays_offense_prior1', 'team_score_prior1', 'opp_pass_att_prior1', 'opp_pass_cmp_prior1', 'opp_pass_yds_prior1', 'opp_pass_td_prior1', 'opp_pass_int_prior1', 'opp_pass_sacked_prior1', 'opp_rush_att_prior1', 'opp_rush_yds_prior1', 'opp_rush_td_prior1', 'opp_first_down_prior1', 'opp_turnovers_prior1', 'opp_penalties_prior1', 'opp_penalties_yds_prior1', 'opp_fga_prior1', 'opp_fgm_prior1', 'opp_xpa_prior1', 'opp_xpm_prior1', 'opp_plays_offense_prior1', 'opp_score_prior1', 'team_pass_yds_roll3', 'team_rush_yds_roll3', 'team_pass_td_roll3', 'team_rush_td_roll3', 'team_turnovers_roll3', 'team_score_roll3', 'team_pass_att_roll3', 'team_rush_att_roll3', 'team_first_down_roll3', 'team_penalties_roll3', 'team_plays_offense_roll3', 'team_pass_sacked_roll3', 'team_fga_roll3', 'opp_pass_yds_roll3', 'opp_rush_yds_roll3', 'opp_pass_td_roll3', 'opp_rush_td_roll3', 'opp_turnovers_roll3', 'opp_score_roll3', 'opp_pass_att_roll3', 'opp_rush_att_roll3', 'opp_first_down_roll3', 'opp_penalties_roll3', 'opp_plays_offense_roll3', 'opp_pass_sacked_roll3', 'team_pass_yds_roll5', 'team_rush_yds_roll5', 'team_pass_td_roll5', 'team_rush_td_roll5', 'team_turnovers_roll5', 'team_score_roll5', 'team_pass_att_roll5', 'team_rush_att_roll5', 'team_first_down_roll5', 'team_penalties_roll5', 'team_plays_offense_roll5', 'team_pass_sacked_roll5', 'team_fga_roll5', 'opp_pass_yds_roll5', 'opp_rush_yds_roll5', 'opp_pass_td_roll5', 'opp_rush_td_roll5', 'opp_turnovers_roll5', 'opp_score_roll5', 'opp_pass_att_roll5', 'opp_rush_att_roll5', 'opp_first_down_roll5', 'opp_penalties_roll5', 'opp_plays_offense_roll5', 'opp_pass_sacked_roll5', 'team_pass_yds_roll10', 'team_rush_yds_roll10', 'team_pass_td_roll10', 'team_rush_td_roll10', 'team_turnovers_roll10', 'team_score_roll10', 'team_pass_att_roll10', 'team_rush_att_roll10', 'team_first_down_roll10', 'team_penalties_roll10', 'team_plays_offense_roll10', 'team_pass_sacked_roll10', 'team_fga_roll10', 'opp_pass_yds_roll10', 'opp_rush_yds_roll10', 'opp_pass_td_roll10', 'opp_rush_td_roll10', 'opp_turnovers_roll10', 'opp_score_roll10', 'opp_pass_att_roll10', 'opp_rush_att_roll10', 'opp_first_down_roll10', 'opp_penalties_roll10', 'opp_plays_offense_roll10', 'opp_pass_sacked_roll10', 'team_pass_yds_season', 'team_rush_yds_season', 'team_pass_td_season', 'team_rush_td_season', 'team_turnovers_season', 'team_score_season', 'team_pass_att_season', 'team_rush_att_season', 'team_first_down_season', 'team_penalties_season', 'team_plays_offense_season', 'team_pass_sacked_season', 'team_fga_season', 'opp_pass_yds_season', 'opp_rush_yds_season', 'opp_pass_td_season', 'opp_rush_td_season', 'opp_turnovers_season', 'opp_score_season', 'opp_pass_att_season', 'opp_rush_att_season', 'opp_first_down_season', 'opp_penalties_season', 'opp_plays_offense_season', 'opp_pass_sacked_season', 'team_pass_yds_ewm', 'team_rush_yds_ewm', 'team_pass_td_ewm', 'team_rush_td_ewm', 'team_turnovers_ewm', 'team_score_ewm', 'team_pass_att_ewm', 'team_rush_att_ewm', 'team_first_down_ewm', 'team_penalties_ewm', 'team_plays_offense_ewm', 'team_pass_sacked_ewm', 'team_fga_ewm', 'opp_pass_yds_ewm', 'opp_rush_yds_ewm', 'opp_pass_td_ewm', 'opp_rush_td_ewm', 'opp_turnovers_ewm', 'opp_score_ewm', 'opp_pass_att_ewm', 'opp_rush_att_ewm', 'opp_first_down_ewm', 'opp_penalties_ewm', 'opp_plays_offense_ewm', 'opp_pass_sacked_ewm', 'team_pass_ypa_roll3', 'team_pass_ypa_roll5', 'team_pass_ypa_roll10', 'team_pass_ypa_season', 'opp_pass_ypa_roll3', 'opp_pass_ypa_roll5', 'opp_pass_ypa_roll10', 'opp_pass_ypa_season', 'team_pass_td_rate_roll3', 'team_pass_td_rate_roll5', 'team_pass_td_rate_roll10', 'team_pass_td_rate_season', 'opp_pass_td_rate_roll3', 'opp_pass_td_rate_roll5', 'opp_pass_td_rate_roll10', 'opp_pass_td_rate_season', 'team_rush_ypc_roll3', 'team_rush_ypc_roll5', 'team_rush_ypc_roll10', 'team_rush_ypc_season', 'opp_rush_ypc_roll3', 'opp_rush_ypc_roll5', 'opp_rush_ypc_roll10', 'opp_rush_ypc_season', 'team_pass_rate_roll3', 'team_pass_rate_roll5', 'team_pass_rate_roll10', 'team_pass_rate_season', 'opp_pass_rate_roll3', 'opp_pass_rate_roll5', 'opp_pass_rate_roll10', 'opp_pass_rate_season', 'team_rush_rate_roll3', 'team_rush_rate_roll5', 'team_rush_rate_roll10', 'team_rush_rate_season', 'opp_rush_rate_roll3', 'opp_rush_rate_roll5', 'opp_rush_rate_roll10', 'opp_rush_rate_season', 'team_fd_rate_roll3', 'team_fd_rate_roll5', 'team_fd_rate_roll10', 'team_fd_rate_season', 'opp_fd_rate_roll3', 'opp_fd_rate_roll5', 'opp_fd_rate_roll10', 'opp_fd_rate_season', 'team_fga_rate_roll3', 'team_fga_rate_roll5', 'team_fga_rate_roll10', 'team_fga_rate_season', 'diff_pass_yds_roll3', 'diff_pass_yds_roll5', 'diff_pass_yds_roll10', 'diff_pass_yds_season', 'diff_rush_yds_roll3', 'diff_rush_yds_roll5', 'diff_rush_yds_roll10', 'diff_rush_yds_season', 'diff_pass_td_roll3', 'diff_pass_td_roll5', 'diff_pass_td_roll10', 'diff_pass_td_season', 'diff_rush_td_roll3', 'diff_rush_td_roll5', 'diff_rush_td_roll10', 'diff_rush_td_season', 'diff_turnovers_roll3', 'diff_turnovers_roll5', 'diff_turnovers_roll10', 'diff_turnovers_season', 'diff_score_roll3', 'diff_score_roll5', 'diff_score_roll10', 'diff_score_season', 'diff_first_down_roll3', 'diff_first_down_roll5', 'diff_first_down_roll10', 'diff_first_down_season', 'diff_pass_ypa_roll3', 'diff_pass_ypa_roll5', 'diff_pass_ypa_roll10', 'diff_pass_ypa_season', 'diff_rush_ypc_roll3', 'diff_rush_ypc_roll5', 'diff_rush_ypc_roll10', 'diff_rush_ypc_season', 'diff_pass_td_rate_roll3', 'diff_pass_td_rate_roll5', 'diff_pass_td_rate_roll10', 'diff_pass_td_rate_season', 'diff_fd_rate_roll3', 'diff_fd_rate_roll5', 'diff_fd_rate_roll10', 'diff_fd_rate_season', 'diff_pass_rate_roll3', 'diff_pass_rate_roll5', 'diff_pass_rate_roll10', 'diff_pass_rate_season', 'diff_rush_rate_roll3', 'diff_rush_rate_roll5', 'diff_rush_rate_roll10', 'diff_rush_rate_season', 'point_diff_roll3', 'point_diff_roll5', 'point_diff_roll10', 'point_diff_season', 'team_win_pre', 'team_games_pre', 'team_win_pct_pre', 'opponent_win_pct_pre', 'team_sos_win_pct_roll3', 'opp_sos_win_pct_roll3', 'team_sos_win_pct_roll5', 'opp_sos_win_pct_roll5', 'team_sos_win_pct_roll10', 'opp_sos_win_pct_roll10', 'team_point_diff_roll3', 'team_point_diff_roll5', 'team_point_diff_roll10', 'league_avg_pd', 'league_point_diff_roll3', 'league_point_diff_roll5', 'league_point_diff_roll10', 'team_adj_point_diff_roll3', 'team_adj_point_diff_roll5', 'team_adj_point_diff_roll10', 'team_win_streak_pre', 'team_loss_streak_pre', 'blowout_rate_roll3', 'close_game_rate_roll3', 'blowout_rate_roll5', 'close_game_rate_roll5', 'blowout_rate_roll10', 'close_game_rate_roll10', 'temp_bin_le_32', 'temp_bin_33_60', 'temp_bin_61_80', 'temp_bin_gt_80', 'extreme_cold', 'wind2', 'is_home_x_temp', 'roof_dome', 'surface_turf', 'week_number', 'week_scaled', 'late_season', 'team_wins_pre', 'must_win_proxy', 'team_fg_per_play_rate_roll3', 'team_fg_per_play_rate_roll5', 'team_fg_per_play_rate_roll10', 'team_pass_ypa_season_z', 'team_rush_ypc_season_z', 'team_fd_rate_season_z', 'int_is_home__team_win_pct_pre', 'int_is_home__diff_pass_ypa_roll5', 'point_diff_prior1', 'team_prob_novig', 'opp_prob_novig', 'rest_days', 'short_rest', 'bye_week', 'team_win_prev1', 'team_games_played_pre', 'team_losses_pre']\n"
=======
      "['game_id', 'season', 'week', 'date', 'team', 'opponent', 'is_home', 'team_score', 'opp_score', 'team_win', 'point_diff', 'home_team', 'away_team', 'home_score', 'away_score', 'Winner', 'spread_line', 'total_line', 'stadium', 'roof', 'surface', 'temp', 'wind', 'game_type', 'weekday', 'gametime', 'location', 'referee', 'team_coach', 'team_fga', 'team_fgm', 'team_first_down', 'team_fumbles_lost', 'team_moneyline', 'team_pass_att', 'team_pass_cmp', 'team_pass_int', 'team_pass_sacked', 'team_pass_td', 'team_pass_yds', 'team_penalties', 'team_penalties_yds', 'team_plays_offense', 'team_rush_att', 'team_rush_td', 'team_rush_yds', 'team_turnovers', 'team_xpa', 'team_xpm', 'opp_coach', 'opp_fga', 'opp_fgm', 'opp_first_down', 'opp_fumbles_lost', 'opp_moneyline', 'opp_pass_att', 'opp_pass_cmp', 'opp_pass_int', 'opp_pass_sacked', 'opp_pass_td', 'opp_pass_yds', 'opp_penalties', 'opp_penalties_yds', 'opp_plays_offense', 'opp_rush_att', 'opp_rush_td', 'opp_rush_yds', 'opp_turnovers', 'opp_xpa', 'opp_xpm', 'team_pass_att_prior1', 'team_pass_cmp_prior1', 'team_pass_yds_prior1', 'team_pass_td_prior1', 'team_pass_int_prior1', 'team_pass_sacked_prior1', 'team_rush_att_prior1', 'team_rush_yds_prior1', 'team_rush_td_prior1', 'team_first_down_prior1', 'team_turnovers_prior1', 'team_penalties_prior1', 'team_penalties_yds_prior1', 'team_fga_prior1', 'team_fgm_prior1', 'team_xpa_prior1', 'team_xpm_prior1', 'team_plays_offense_prior1', 'team_score_prior1', 'opp_pass_att_prior1', 'opp_pass_cmp_prior1', 'opp_pass_yds_prior1', 'opp_pass_td_prior1', 'opp_pass_int_prior1', 'opp_pass_sacked_prior1', 'opp_rush_att_prior1', 'opp_rush_yds_prior1', 'opp_rush_td_prior1', 'opp_first_down_prior1', 'opp_turnovers_prior1', 'opp_penalties_prior1', 'opp_penalties_yds_prior1', 'opp_fga_prior1', 'opp_fgm_prior1', 'opp_xpa_prior1', 'opp_xpm_prior1', 'opp_plays_offense_prior1', 'opp_score_prior1', 'team_prob_novig', 'opp_prob_novig', 'rest_days', 'short_rest', 'bye_week', 'winrate_l3', 'winrate_l5']\n"
>>>>>>> 45e88afe4356929898abe4042ae8d89d57bb12c6
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
